{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256fb92e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "import pytorch_lightning\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    ")\n",
    "\n",
    "from monai.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import numpy as np\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torchmetrics import F1Score\n",
    "from torch.optim.lr_scheduler import SequentialLR, LambdaLR, StepLR, SequentialLR\n",
    "import ssl\n",
    "\n",
    "from random import shuffle\n",
    "import os\n",
    "import random\n",
    "\n",
    "from torchmetrics.classification import BinaryAUROC\n",
    "\n",
    "import glob\n",
    "import cv2\n",
    "from skimage.filters import threshold_otsu\n",
    "from scipy.stats import kurtosis, skew\n",
    "\n",
    "import scipy\n",
    "import scipy.ndimage as ndi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f8e82c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class Net(pytorch_lightning.LightningModule):\n",
    "    def __init__(self, model, optimizer, scheduler, train_loader, val_loader):\n",
    "        super().__init__()\n",
    "        self._model = model\n",
    "        self._optimizer = optimizer\n",
    "        self._scheduler = scheduler\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.loss_function = BCEWithLogitsLoss()\n",
    "        self.metric = F1Score(task='binary')\n",
    "        self.activation = Activations(sigmoid=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._model(x)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.val_loader\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': scheduler}\n",
    "    \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        labels = labels[:, None]\n",
    "        output = self.forward(images)\n",
    "        loss = self.loss_function(output, labels.float())\n",
    "        self.log_dict({\"training_loss\": loss})\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        labels = labels[:, None]\n",
    "        output = self.forward(images)\n",
    "        loss = self.loss_function(output, labels.float())\n",
    "        metric = self.metric(self.activation(output), labels.float())\n",
    "        self.log_dict({\"f1\":metric, \"loss\": loss})\n",
    "        return {\"loss\": loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8852a72",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, file_list, labels=None, transform=None):\n",
    "        self.file_list = file_list\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.file_list[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        if self.labels is not None:\n",
    "            label = self.labels[idx]\n",
    "            return img, label\n",
    "        else:\n",
    "            return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e7a714",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_transforms = T.Compose([\n",
    "    T.Resize(224, interpolation=InterpolationMode.BICUBIC),\n",
    "    T.RandomResizedCrop(224),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomVerticalFlip(),\n",
    "    T.RandomRotation(20),\n",
    "    T.GaussianBlur(kernel_size=(7, 13), sigma=(0.1, 1.0)),\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_transforms = T.Compose([\n",
    "    T.Resize(224, interpolation=InterpolationMode.BICUBIC),\n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2b2902",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_train_valid_split():\n",
    "    '''\n",
    "    Creates train/valid split for cnn. \n",
    "    Return train_paths, valid_paths, train_labels, valid_labels.\n",
    "    '''\n",
    "    \n",
    "    base_dir = '/kaggle/input/ai-vs-human-generated-dataset'\n",
    "    train_csv = '/kaggle/input/ai-vs-human-generated-dataset/train.csv'\n",
    "    \n",
    "    \n",
    "    df_train = pd.read_csv(train_csv)\n",
    "    \n",
    "    \n",
    "    df_train = df_train[['file_name', 'label']]\n",
    "    \n",
    "    df_train['file_name'] = df_train['file_name'].apply(lambda x: os.path.join(base_dir, x))\n",
    "    all_image_paths = df_train['file_name'].values\n",
    "    all_labels = df_train['label'].values\n",
    "    train_paths, val_paths, train_labels, val_labels = train_test_split(all_image_paths, \n",
    "                                                                        all_labels, test_size=0.05,        \n",
    "                                                                        random_state=43,\n",
    "                                                                        shuffle=False)\n",
    "    return train_paths, val_paths, train_labels, val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e819504e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def create_dataloaders(train_paths, val_paths, train_labels, val_labels):\n",
    "    \"\"\"\n",
    "    Returns train and valid dataloader.\n",
    "    \"\"\"\n",
    "    batch_size = 32\n",
    "    train_data = ImageDataset(train_paths, train_labels, transform=train_transforms)\n",
    "    val_data   = ImageDataset(val_paths,   val_labels,   transform=val_transforms)\n",
    "    train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True,  num_workers=4)\n",
    "    val_loader   = DataLoader(dataset=val_data,   batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeec835",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def create_model_optimizer_scheduler():\n",
    "    \"\"\"\n",
    "    Returns model, optimizer and scheduler\n",
    "    \"\"\"\n",
    "\n",
    "    model = models.convnext_base(weights=\"DEFAULT\")\n",
    "    \n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    for param in model.features[-2:].parameters(): \n",
    "        param.requires_grad = True\n",
    "    \n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.AdaptiveAvgPool2d((1, 1)),  \n",
    "        nn.Flatten(),                  \n",
    "        nn.BatchNorm1d(1024),          \n",
    "        nn.Linear(1024, 512),          \n",
    "        nn.ReLU(),                     \n",
    "        nn.Dropout(0.4),               \n",
    "        nn.Linear(512, 1)              \n",
    "    )\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {'params': model.features[-2:].parameters(), 'lr': 1e-5},\n",
    "        {'params': model.classifier.parameters(), 'lr': 1e-4}     \n",
    "    ])\n",
    "    \n",
    "    scheduler = StepLR(optimizer, step_size=5, gamma=0.7)\n",
    "\n",
    "    return model, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40c9446",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, scheduler, train_loader, val_loader):\n",
    "    \"\"\"\n",
    "    Trains model\n",
    "    \"\"\"\n",
    "    net = Net(model, optimizer, scheduler, train_loader, val_loader)\n",
    "    trainer = pytorch_lightning.Trainer(\n",
    "            devices=[0],\n",
    "            max_epochs=5,\n",
    "            enable_checkpointing=True,\n",
    "            num_sanity_val_steps=1,\n",
    "            log_every_n_steps=16,\n",
    "            callbacks=[ModelCheckpoint('models/', '{f1:.2f}_{epoch}', monitor='f1', mode='max')]\n",
    "        )\n",
    "    trainer.fit(net)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed63924",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_paths, val_paths, train_labels, val_labels = get_train_valid_split()\n",
    "\n",
    "train_loader, val_loader = create_dataloaders(train_paths, val_paths, train_labels, val_labels)\n",
    "\n",
    "model, optimizer, scheduler = create_model_optimizer_scheduler()\n",
    "\n",
    "net = train_model(model, optimizer, scheduler, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9d65da",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Paths\n",
    "MODEL_PATH = \"/kaggle/working/best_model_mobilenetv3\"\n",
    "TEST_CSV_PATH = \"/kaggle/input/ai-vs-human-generated-dataset/test.csv\"\n",
    "TEST_IMAGES_PATH = \"/kaggle/input/ai-vs-human-generated-dataset/test_data_v2\"\n",
    "OUTPUT_CSV_PATH = \"/kaggle/working/submission.csv\"\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model(MODEL_PATH)\n",
    "\n",
    "# Load the test CSV\n",
    "test_df = pd.read_csv(TEST_CSV_PATH)\n",
    "\n",
    "# Image preprocessing function\n",
    "def preprocess_image(img_path, target_size=(224, 224)):\n",
    "    img = image.load_img(img_path, target_size=target_size)\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    #img_array /= 255.0  # Normalize\n",
    "    return img_array\n",
    "\n",
    "# Make predictions\n",
    "predictions = []\n",
    "for img_path in test_df['id']:\n",
    "    img_filename = os.path.basename(img_path)  # Extract only the filename\n",
    "    full_img_path = os.path.join(TEST_IMAGES_PATH, img_filename)\n",
    "\n",
    "    if not os.path.exists(full_img_path):  # Debugging check\n",
    "        print(f\"File not found: {full_img_path}\")\n",
    "\n",
    "    img_array = preprocess_image(full_img_path)\n",
    "    pred = model.predict(img_array)[0][0]  # Get prediction score\n",
    "    label = 0 if pred > 0.5 else 1\n",
    "    predictions.append(label)\n",
    "\n",
    "# Save predictions to CSV\n",
    "submission_df = pd.DataFrame({'id': test_df['id'], 'label': predictions})"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
